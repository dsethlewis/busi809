---
title: "HW #7: Multiple Dependent Variables"
author: "Daniel Lewis"
date: "2/12/20"
output: 
  github_document:
    pandoc_args: --webtex
bibliography: ../biblio809.bib
link-citations: true
csl: ../apa.csl
---

## Set up working environment

As usual, I need to load the R packages I will be using.

```{r message = F, warning = F}
# R Markdown
library(knitr)

# Project
library(here)

# Statistics
library(CCA)
library(psych)

# Tidyverse
library(readxl)
library(broom)
library(tidyverse)
```

# 1. Reproducing the Thompson example

## Importing the data

First, I will read in the Thompson data. I will also glance at the data to see what I'm working with.

```{r}
tbl.1 <- read_excel(here('data', 'THOMPSON.xlsx'))

tbl.1[1:10, ] %>%
  kable

describe(tbl.1)[c('n', 'mean', 'sd', 'min', 'max')] %>%
  kable
```

Aside from the ridiculously low $n$, and one person with a low enough IQ that it could impair functioning during data collection, the data look alright.

## Preliminary steps

Now, I will attempt to reproduce the canonical correlation analysis (CCA) presented by @thompson1991m&eic&d. Unfortunately, I found Thompson's explanation practically indecipherable, and @sherry2005jpa provide only the SPSS syntax without explanation, so I fell back on using the R package ```CCA``` to automate the process [@gonzalez2008jss].

```CCA``` first requires that dataframe subsets be transformed into matrices.

```{r}
mtx.1.X <- as.matrix(tbl.1[1:3])
mtx.1.Y <- as.matrix(tbl.1[4:6])
```

Then, I need to verify that zero-order correlations are sufficiently high to warrant continue with CCA.

```{r}
lst.1.cor <- matcor(mtx.1.X, mtx.1.Y)
lst.1.cor

img.matcor(lst.1.cor, type = 2)
```

While a few correlations are very low (e.g., CHA6 with INT6), most of the correlations are substantial. I think we have more than enough to work with.

## Canonical correlation analysis

Finally, I can compute the canonical correlations.

```{r}
lst.1.cca <- cc(mtx.1.X, mtx.1.Y)
```

Results in hand, I will try to match the tables to the results presented in @thompson1991m&eic&d.

The first columns (corresponding to Function I) of ```xscores``` and ```yscores``` approximate the results in Table 2 from @thompson1991m&eic&d under CRIT1 and PRED1 respectively.

```{r}
tibble(lst.1.cca$scores[["xscores"]][,1],
       lst.1.cca$scores[["yscores"]][,1]) %>%
  kable(col.names = c("xscores", "yscores"), digits = 5)
```

Next, the results for ```corr.X.xscores``` and ```corr.Y.yscores``` approximate the results of the columns labeled "Str." in Table 3. I have pulled out the results for Function I as an example, but I checked the Function II and Function III results for completeness.

```{r}
tibble(c("CHA", "INT", "OTH"),
       lst.1.cca$scores[["corr.X.xscores"]][,1],
       lst.1.cca$scores[["corr.Y.yscores"]][,1]) %>%
  kable(col.names = c("Variable", "Str. (X &#xFF5C; 6)", "Str. (Y &#xFF5C; 2)"), digits = 4)
```

At first I was worried because the canonical correlations do *not* seem to match. But then I realized that ```CCA``` produced unmodified correlations while @thompson1991m&eic&d presented only squared correlations.

```{r}
tibble(Function = c("I", "II", "III"),
       Rc = lst.1.cca$cor,
       Rc2 = lst.1.cca$cor^2) %>%
  kable(digits = 4)
```

To substantively interpret the final results, we can say that the data from first scale explains about 87% of the variance in the data from the second scale.

# 2. Multivariate Multiple Regression Analysis

As before, I will begin by importing and viewing the data.

```{r}
tbl.2 <- read_excel(here('data', 'TAHEALTH.xlsx'))

tbl.2[1:10, ] %>%
  kable

describe(tbl.2)[c('n', 'mean', 'sd', 'min', 'max')] %>%
  kable
```

Next, I will compute a multivariate multiple regression (MMR) analysis, as described in @dwyer1983smftsabs.

```{r}
fit.2 <- lm(cbind(ANX, DEP, SOM, ANG) ~ BORSI + BORHC + FRATP + FRAHC, tbl.2)
tidy(fit.2) %>%
  kable
```

If we interpret the general hypothesis that "the components of Type-A behavior are detrimental to health" as meaning that at least one $b$ (excepting $b_0$) is significantly different from zero, the hypothesis is clearly supported. Only FRAHC does not significantly predict any health outcome (controlling for other predictors; $\alpha = .05$). This interpretation is equivalent to Hypothesis 1 described by @dwyer1983smftsabs.

Alternatively, we might hypothesize that all four independent variables together predict each of the four health outcomes. To consider that hypothesis, I will pull together the $R^2$ values for the four models.

```{r}
fit.2 %>%
  summary %>%
  map_dbl("r.squared") %>%
  enframe %>%
  rename(outcome = "name", R2 = "value") %>%
  mutate(outcome = str_trunc(outcome, 3, "left", ellipsis = "")) %>%
  kable(digits = 3)
```

The Type-A characteristics best predict anxiety, but even that has an $R^2$ of only about 0.19, meaning that the Type-A predictors explain about 19% of the variance in anxiety in the sample. The other models have even poorer fit. However, all four models significantly fit the data.

A third hypothesis could be that each independent variable predicts all four health outcomes. To test this hypothesis, I will compute a MANOVA.

```{r}
manova(cbind(ANX, DEP, SOM, ANG) ~ BORSI + BORHC + FRATP + FRAHC, tbl.2) %>%
  tidy %>%
  kable
```

R uses Pillai's trace and its corresponding F-statistic as the test statistic for each independent variable. The results suggest that BORSI and FRATP significantly predict health outcomes, while BORHC and FRAHC do not.

# 3. Canonical Correlation Analysis

In order to calculate Wilks' lambda and Bartlett's Chi-Square as an omnibus test, I need the four canonical correlations.

```{r}
mtx.2.X <- as.matrix(tbl.2[5:8])
mtx.2.Y <- as.matrix(tbl.2[1:4])
lst.2 <- cc(mtx.2.X, mtx.2.Y)$cor^2

tibble(Function = c("I", "II", "III", "IV"),
       Rc2 = lst.2) %>%
  kable(digits = 4)
```

## a. Wilks' lambda

I will write a function to calculate Wilks' lambda ($W$) and compute $W$ for the first canonical correlation.

```{r}
# This function takes a vector of canonical correlations and computes Wilks' lambda
fun.2.a <- function(ccs) {
  ccs %>%
    map_dbl(~ 1 - .) %>%
    prod
}

fun.2.a(lst.2)
```

## b. Bartlett's Chi-Square

I will write a second function that uses $W$ to calculate a $\chi^2$-statistic and compute $\chi^2$ for the first canonical correlation.

```{r}
# This function takes the sample size (N), number of IVs (p), the number of DVs (q),
# and Wilks' lambda and computes Bartlett's Chi-Square (and a p-value)
fun.2.b <- function(N, p, q, W) {
  c <- -(N - 1 - .5 * (p + q + 1)) * log(W)
  c(chi.sq = c,
    p.val = pchisq(c, p*q, lower.tail = F))
}

fun.2.b(211, 4, 4, fun.2.a(lst.2))
```

Clearly, that is significant, so I will proceed to test the remaining canonical correlations.

```{r}
# This function takes a vector of canonical correlations and the sample size and returns
# all significant correlations, plus one
fun.2 <- function(ccs, N) {
  prob <- 0.0
  res <- tibble(ord = integer(), Rc2 = numeric(), W = numeric(),
                chi.sq = numeric(), p.val = numeric(), .rows = 0)
  while (prob < .05 && length(ccs) > 0) {
    w <- fun.2.a(ccs)
    c <- fun.2.b(N, length(ccs), length(ccs), w)
    prob <- c[[2]]
    res <- add_row(res, ord = 5 - length(ccs), Rc2 = ccs[[1]],
                   W = w, chi.sq = c[[1]], p.val = prob)
    ccs <- ccs[-1]
  }
  res
}

fun.2(lst.2, 211) %>%
  kable
```

It appears that only the first canonical correlation is significant ($R_c^2 \approx 0.2, p < .001$). Therefore, we can reject the null hypothesis and conclude that there is a relationship between Type-A characteristics and health outcomes as represented by the full model. However, because the second canonical correlation was not significant, we cannot say anything about the subsets of the variable sets.

At least, not according to the procedure described in the syllabus and in @thompson1991m&eic&d. @sherry2005jpa, by contrast, propose exploring all possible subsets of the variables. They write

> There will be as many functions (i.e., variates) as there are variables in the smaller set, which in this case is four (the predictor set). Each function must be evaluated because some of them may not explain enough of the relationship between the variable sets to warrant interpretation.
>
> --- @sherry2005jpa [p.42]

I do not quite see what the use would be in testing the third and fourth canonical correlations, which have even less variance to work with, when the second correlation was not significant. But perhaps it would bring into question the value of the additional predictors.

# 4. Comparison of MMR and CCA

MMR offers more information about the unique effects of each predictor on the set of outcomes and on each outcome individually. It does allow for some correction to avoid "experimentwise" Type I error. However, it does not allow for a single test of the full model including the relationships between all independent and dependent variables.

CCA provides a simple test of the full model, as well as allowing for analysis of any residual variance that can be explained by a subset of the full model. However, obtaining the unique effects of each predictor requires additional calculations and would be easier found through MMR.

# References